---
language:
- zh
- en
tags:
- qwen
pipeline_tag: text-generation
inference: false
---

# Qwen-Audio-Chat

<br>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/audio_logo.jpg" width="400"/>
<p>
<br>

<p align="center">
        Qwen-Audio <a href="https://www.modelscope.cn/models/qwen/QWen-Audio/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-Audio">ğŸ¤—</a>&nbsp ï½œ Qwen-Audio-Chat <a href="https://www.modelscope.cn/models/qwen/QWen-Audio-Chat/summary">ğŸ¤– <a>| <a href="https://huggingface.co/Qwen/Qwen-Audio-Chat">ğŸ¤—</a>&nbsp | &nbsp&nbsp Demo<a href="https://modelscope.cn/studios/qwen/Qwen-Audio-Chat-Demo/summary"> ğŸ¤–</a> | <a href="https://huggingface.co/spaces/Qwen/Qwen-Audio">ğŸ¤—</a>&nbsp
<br>
&nbsp&nbsp<a href="https://qwen-audio.github.io/Qwen-Audio/">Homepage</a>&nbsp ï½œ &nbsp<a href="http://arxiv.org/abs/2311.07919">Paper</a>
</p>
<br><br>

**Qwen-Audio** (Qwen Large Audio Language Model) is the multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-Audio accepts diverse audio (human speech, natural sound, music and song) and text as inputs, outputs text. The contribution of Qwen-Audio include:

- **Fundamental audio models**: Qwen-Audio is a fundamental multi-task audio-language model that supports various tasks, languages, and audio types, serving as a universal audio understanding model. Building upon Qwen-Audio, we develop Qwen-Audio-Chat through instruction fine-tuning, enabling multi-turn dialogues and supporting diverse audio-oriented scenarios.
- **Multi-task learning framework for all types of audios**: To scale up audio-language pre-training, we address the challenge of variation in textual labels associated with different datasets by proposing a multi-task training framework, enabling knowledge sharing and avoiding one-to-many interference. Our model incorporates more than 30 tasks and extensive experiments show the model achieves strong performance.
- **Strong Performance**: Experimental results show that Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Specifically, Qwen-Audio achieves state-of-the-art results on the test set of Aishell1, cochlscene, ClothoAQA, and VocalSound.
- **Flexible multi-run chat from audio and text input**: Qwen-Audio supports multiple-audio analysis, sound understading and reasoning, music appreciation, and tool usage for speech editing.

**Qwen-Audio** æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„å¤§è§„æ¨¡éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLarge Audio Language Modelï¼‰ã€‚Qwen-Audio å¯ä»¥ä»¥å¤šç§éŸ³é¢‘ (åŒ…æ‹¬è¯´è¯äººè¯­éŸ³ã€è‡ªç„¶éŸ³ã€éŸ³ä¹ã€æ­Œå£°ï¼‰å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»¥æ–‡æœ¬ä½œä¸ºè¾“å‡ºã€‚Qwen-Audio ç³»åˆ—æ¨¡å‹çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼š

- **éŸ³é¢‘åŸºçŸ³æ¨¡å‹**ï¼šQwen-Audioæ˜¯ä¸€ä¸ªæ€§èƒ½å“è¶Šçš„é€šç”¨çš„éŸ³é¢‘ç†è§£æ¨¡å‹ï¼Œæ”¯æŒå„ç§ä»»åŠ¡ã€è¯­è¨€å’ŒéŸ³é¢‘ç±»å‹ã€‚åœ¨Qwen-Audioçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡æŒ‡ä»¤å¾®è°ƒå¼€å‘äº†Qwen-Audio-Chatï¼Œæ”¯æŒå¤šè½®ã€å¤šè¯­è¨€ã€å¤šè¯­è¨€å¯¹è¯ã€‚Qwen-Audioå’ŒQwen-Audio-Chatæ¨¡å‹å‡å·²å¼€æºã€‚
- **å…¼å®¹å¤šç§å¤æ‚éŸ³é¢‘çš„å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶**ï¼šä¸ºäº†é¿å…ç”±äºæ•°æ®æ”¶é›†æ¥æºä¸åŒä»¥åŠä»»åŠ¡ç±»å‹ä¸åŒï¼Œå¸¦æ¥çš„éŸ³é¢‘åˆ°æ–‡æœ¬çš„ä¸€å¯¹å¤šçš„å¹²æ‰°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡è®­ç»ƒæ¡†æ¶ï¼Œå®ç°ç›¸ä¼¼ä»»åŠ¡çš„çŸ¥è¯†å…±äº«ï¼Œå¹¶å°½å¯èƒ½å‡å°‘ä¸åŒä»»åŠ¡ä¹‹é—´çš„å¹²æ‰°ã€‚é€šè¿‡æå‡ºçš„æ¡†æ¶ï¼ŒQwen-Audioå¯ä»¥å®¹çº³è®­ç»ƒè¶…è¿‡30å¤šç§ä¸åŒçš„éŸ³é¢‘ä»»åŠ¡ï¼›
- **å‡ºè‰²çš„æ€§èƒ½**ï¼šQwen-Audioåœ¨ä¸éœ€è¦ä»»ä½•ä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨å„ç§åŸºå‡†ä»»åŠ¡ä¸Šå–å¾—äº†é¢†å…ˆçš„ç»“æœã€‚å…·ä½“å¾—ï¼ŒQwen-Audioåœ¨Aishell1ã€cochlsceneã€ClothoAQAå’ŒVocalSoundçš„æµ‹è¯•é›†ä¸Šéƒ½è¾¾åˆ°äº†SOTAï¼›
- **æ”¯æŒå¤šè½®éŸ³é¢‘å’Œæ–‡æœ¬å¯¹è¯ï¼Œæ”¯æŒå„ç§è¯­éŸ³åœºæ™¯**ï¼šQwen-Audio-Chatæ”¯æŒå£°éŸ³ç†è§£å’Œæ¨ç†ã€éŸ³ä¹æ¬£èµã€å¤šéŸ³é¢‘åˆ†æã€å¤šè½®éŸ³é¢‘-æ–‡æœ¬äº¤é”™å¯¹è¯ä»¥åŠå¤–éƒ¨è¯­éŸ³å·¥å…·çš„ä½¿ç”¨(å¦‚è¯­éŸ³ç¼–è¾‘)ã€‚


We release Qwen-Audio and Qwen-Audio-Chat, which are pretrained model and Chat model respectively. For more details about Qwen-Audio, please refer to our [Github Repo](https://github.com/QwenLM/Qwen-Audio/tree/main). This repo is the one for Qwen-Audio-Chat.
<br>

ç›®å‰ï¼Œæˆ‘ä»¬æä¾›äº†Qwen-Audioå’ŒQwen-Audio-Chatä¸¤ä¸ªæ¨¡å‹ï¼Œåˆ†åˆ«ä¸ºé¢„è®­ç»ƒæ¨¡å‹å’ŒChatæ¨¡å‹ã€‚å¦‚æœæƒ³äº†è§£æ›´å¤šå…³äºä¿¡æ¯ï¼Œè¯·ç‚¹å‡»[é“¾æ¥](https://github.com/QwenLM/Qwen-Audio/tree/main)æŸ¥çœ‹Githubä»“åº“ã€‚æœ¬ä»“åº“ä¸ºQwen-Audio-Chatä»“åº“ã€‚


## Requirements
* python 3.8 and above
* pytorch 1.12 and above, 2.0 and above are recommended
* CUDA 11.4 and above are recommended (this is for GPU users)
* FFmpeg
  <br>

## Quickstart
Below, we provide simple examples to show how to use Qwen-Audio with ğŸ¤— Transformers.

Before running the code, make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries.

```bash
pip install -r requirements.txt
```
Now you can start with Transformers. For more usage, please refer to [tutorial](https://github.com/QwenLM/Qwen-Audio/blob/main/TUTORIAL.md).

#### ğŸ¤— Transformers

To use Qwen-Audio for the inference, all you need to do is to input a few lines of codes as demonstrated below. However, **please make sure that you are using the latest code.**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

# Note: The default behavior now has injection attack prevention off.
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-Audio-Chat", trust_remote_code=True)

# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-Audio-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-Audio-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-Audio-Chat", device_map="cpu", trust_remote_code=True).eval()
# use cuda device
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-Audio-Chat", device_map="cuda", trust_remote_code=True).eval()

# Specify hyperparameters for generation (No need to do this if you are using transformers>4.32.0)
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-Audio-Chat", trust_remote_code=True)

# 1st dialogue turn
query = tokenizer.from_list_format([
    {'audio': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/1272-128104-0000.flac'}, # Either a local path or an url
    {'text': 'what does the person say?'},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
# The person says: "mister quilter is the apostle of the middle classes and we are glad to welcome his gospel".

# 2nd dialogue turn
response, history = model.chat(tokenizer, 'Find the start time and end time of the word "middle classes"', history=history)
print(response)
# The word "middle classes" starts at <|2.33|> seconds and ends at <|3.26|> seconds.
```


## License Agreement
Researchers and developers are free to use the codes and model weights of Qwen-Audio-Chat. We also allow its commercial use. Check our license at [LICENSE](https://github.com/QwenLM/Qwen-Audio/blob/main/LICENSE.txt) for more details.
<br>

## Citation
If you find our paper and code useful in your research, please consider giving a star and citation

```BibTeX
@article{Qwen-Audio,
  title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},
  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.07919},
  year={2023}
}
```
<br>

## Contact Us

If you are interested to leave a message to either our research team or product team, feel free to send an email to qianwen_opensource@alibabacloud.com.

