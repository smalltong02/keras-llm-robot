---
language:
- zh
- en
tags:
- glm
- visualglm
- chatglm
- thudm
---
# VisualGLM-6B
<p align="center">
   ğŸ’» <a href="https://github.com/THUDM/VisualGLM-6B" target="_blank">Github Repo</a> â€¢ ğŸ¦ <a href="https://twitter.com/thukeg" target="_blank">Twitter</a> â€¢ ğŸ“ƒ <a href="https://arxiv.org/abs/2103.10360" target="_blank">[GLM@ACL 22]</a> <a href="https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> â€¢ ğŸ“ƒ <a href="https://arxiv.org/abs/2210.02414" target="_blank">[GLM-130B@ICLR 23]</a> <a href="https://github.com/THUDM/GLM-130B" target="_blank">[GitHub]</a> <br>
</p>

<p align="center">
    ğŸ‘‹ Join our <a href="https://join.slack.com/t/chatglm/shared_invite/zt-1th2q5u69-7tURzFuOPanmuHy9hsZnKA" target="_blank">Slack</a> and <a href="https://github.com/THUDM/ChatGLM-6B/blob/main/resources/WECHAT.md" target="_blank">WeChat</a>
</p>

## ä»‹ç»
VisualGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ï¼Œæ”¯æŒ**å›¾åƒã€ä¸­æ–‡å’Œè‹±æ–‡**çš„å¤šæ¨¡æ€å¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œè¯­è¨€æ¨¡å‹åŸºäº [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ï¼›å›¾åƒéƒ¨åˆ†é€šè¿‡è®­ç»ƒ [BLIP2-Qformer](https://arxiv.org/abs/2301.12597) æ„å»ºèµ·è§†è§‰æ¨¡å‹ä¸è¯­è¨€æ¨¡å‹çš„æ¡¥æ¢ï¼Œæ•´ä½“æ¨¡å‹å…±78äº¿å‚æ•°ã€‚

VisualGLM-6B ä¾é æ¥è‡ªäº [CogView](https://arxiv.org/abs/2105.13290) æ•°æ®é›†çš„30Mé«˜è´¨é‡ä¸­æ–‡å›¾æ–‡å¯¹ï¼Œä¸300Mç»è¿‡ç­›é€‰çš„è‹±æ–‡å›¾æ–‡å¯¹è¿›è¡Œé¢„è®­ç»ƒï¼Œä¸­è‹±æ–‡æƒé‡ç›¸åŒã€‚è¯¥è®­ç»ƒæ–¹å¼è¾ƒå¥½åœ°å°†è§†è§‰ä¿¡æ¯å¯¹é½åˆ°ChatGLMçš„è¯­ä¹‰ç©ºé—´ï¼›ä¹‹åçš„å¾®è°ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨é•¿è§†è§‰é—®ç­”æ•°æ®ä¸Šè®­ç»ƒï¼Œä»¥ç”Ÿæˆç¬¦åˆäººç±»åå¥½çš„ç­”æ¡ˆã€‚

## è½¯ä»¶ä¾èµ–

```shell
pip install SwissArmyTransformer>=0.3.6 torch>1.10.0 torchvision transformers>=4.27.1 cpm_kernels
```

## ä»£ç è°ƒç”¨ 

å¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ VisualGLM-6B æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š

```ipython
>>> from transformers import AutoTokenizer, AutoModel
>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/visualglm-6b", trust_remote_code=True)
>>> model = AutoModel.from_pretrained("THUDM/visualglm-6b", trust_remote_code=True).half().cuda()
>>> image_path = "your image path"
>>> response, history = model.chat(tokenizer, image_path, "æè¿°è¿™å¼ å›¾ç‰‡ã€‚", history=[])
>>> print(response)
>>> response, history = model.chat(tokenizer, image_path, "è¿™å¼ å›¾ç‰‡å¯èƒ½æ˜¯åœ¨ä»€ä¹ˆåœºæ‰€æ‹æ‘„çš„ï¼Ÿ", history=history)
>>> print(response)
```

å…³äºæ›´å¤šçš„ä½¿ç”¨è¯´æ˜ï¼ŒåŒ…æ‹¬å¦‚ä½•è¿è¡Œå‘½ä»¤è¡Œå’Œç½‘é¡µç‰ˆæœ¬çš„ DEMOï¼Œä»¥åŠä½¿ç”¨æ¨¡å‹é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ [Github Repo](https://github.com/THUDM/VisualGLM-6B)ã€‚

For more instructions, including how to run CLI and web demos, and model quantization, please refer to our [Github Repo](https://github.com/THUDM/VisualGLM-6B).

## åè®®

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE) åè®®å¼€æºï¼ŒVisualGLM-6B æ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª [Model License](MODEL_LICENSE)ã€‚

## å¼•ç”¨

å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡ï¼š

```
@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}
```
```
@article{ding2021cogview,
  title={Cogview: Mastering text-to-image generation via transformers},
  author={Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19822--19835},
  year={2021}
}
```