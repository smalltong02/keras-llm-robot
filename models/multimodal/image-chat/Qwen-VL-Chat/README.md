---
language:
- zh
- en
tags:
- qwen
pipeline_tag: text-generation
inference: false
---

# Qwen-VL-Chat

<br>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_vl.jpg" width="400"/>
<p>
<br>

<p align="center">
        Qwen-VL <a href="https://modelscope.cn/models/qwen/Qwen-VL/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-VL">ğŸ¤—</a>&nbsp ï½œ Qwen-VL-Chat <a href="https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary">ğŸ¤– <a>| <a href="https://huggingface.co/Qwen/Qwen-VL-Chat">ğŸ¤—</a>&nbsp ï½œ Qwen-VL-Chat-Int4 <a href="https://huggingface.co/Qwen/Qwen-VL-Chat-Int4">ğŸ¤—</a>
<br>
<a href="assets/wechat.png">WeChat</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary">Demo</a>&nbsp ï½œ &nbsp<a href="https://arxiv.org/abs/2308.12966">Report</a>
</p>
<br>

**Qwen-VL** æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLarge Vision Language Model, LVLMï¼‰ã€‚Qwen-VL å¯ä»¥ä»¥å›¾åƒã€æ–‡æœ¬ã€æ£€æµ‹æ¡†ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»¥æ–‡æœ¬å’Œæ£€æµ‹æ¡†ä½œä¸ºè¾“å‡ºã€‚Qwen-VL ç³»åˆ—æ¨¡å‹æ€§èƒ½å¼ºå¤§ï¼Œå…·å¤‡å¤šè¯­è¨€å¯¹è¯ã€å¤šå›¾äº¤é”™å¯¹è¯ç­‰èƒ½åŠ›ï¼Œå¹¶æ”¯æŒä¸­æ–‡å¼€æ”¾åŸŸå®šä½å’Œç»†ç²’åº¦å›¾åƒè¯†åˆ«ä¸ç†è§£ã€‚

**Qwen-VL** (Qwen Large Vision Language Model) is the visual multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-VL accepts image, text, and bounding box as inputs, outputs text and bounding box. The features of Qwen-VL include:

ç›®å‰ï¼Œæˆ‘ä»¬æä¾›äº†Qwen-VLå’ŒQwen-VL-Chatä¸¤ä¸ªæ¨¡å‹ï¼Œåˆ†åˆ«ä¸ºé¢„è®­ç»ƒæ¨¡å‹å’ŒChatæ¨¡å‹ã€‚å¦‚æœæƒ³äº†è§£æ›´å¤šå…³äºæ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·ç‚¹å‡»[é“¾æ¥](https://github.com/QwenLM/Qwen-VL/blob/master/visual_memo.md)æŸ¥çœ‹æˆ‘ä»¬çš„æŠ€æœ¯å¤‡å¿˜å½•ã€‚æœ¬ä»“åº“ä¸ºQwen-VL-Chatä»“åº“ã€‚

We release Qwen-VL and Qwen-VL-Chat, which are pretrained model and Chat model respectively. For more details about Qwen-VL, please refer to our [technical memo](https://github.com/QwenLM/Qwen-VL/blob/master/visual_memo.md). This repo is the one for Qwen-VL-Chat.
<br>

## å®‰è£…è¦æ±‚ (Requirements)

* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬
* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
* python 3.8 and above
* pytorch 1.12 and above, 2.0 and above are recommended
* CUDA 11.4 and above are recommended (this is for GPU users)
  <br>

## å¿«é€Ÿå¼€å§‹ (Quickstart)

æˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ ğŸ¤— Transformers å¿«é€Ÿä½¿ç”¨Qwen-VL-Chatã€‚

åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»é…ç½®å¥½ç¯å¢ƒå¹¶å®‰è£…å¥½ç›¸å…³çš„ä»£ç åŒ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç¡®ä¿ä½ æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œç„¶åå®‰è£…ç›¸å…³çš„ä¾èµ–åº“ã€‚

Below, we provide simple examples to show how to use Qwen-VL-Chat with ğŸ¤— Transformers.

Before running the code, make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries.

```bash
pip install -r requirements.txt
```

æ¥ä¸‹æ¥ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨Transformersæ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚å…³äºè§†è§‰æ¨¡å—çš„æ›´å¤šç”¨æ³•ï¼Œè¯·å‚è€ƒ[æ•™ç¨‹](TUTORIAL.md)ã€‚

Now you can start with Transformers. More usage aboue vision encoder, please refer to [tutorial](TUTORIAL_zh.md).

#### ğŸ¤— Transformers

To use Qwen-VL-Chat for the inference, all you need to do is to input a few lines of codes as demonstrated below. However, **please make sure that you are using the latest code.**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

# Note: The default behavior now has injection attack prevention off.
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cpu", trust_remote_code=True).eval()
# use cuda device
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cuda", trust_remote_code=True).eval()

# Specify hyperparameters for generation (No need to do this if you are using transformers>=4.32.0)
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# 1st dialogue turn
query = tokenizer.from_list_format([
    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},
    {'text': 'è¿™æ˜¯ä»€ä¹ˆ'},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
# å›¾ä¸­æ˜¯ä¸€åå¹´è½»å¥³å­åœ¨æ²™æ»©ä¸Šå’Œå¥¹çš„ç‹—ç©è€ï¼Œç‹—çš„å“ç§å¯èƒ½æ˜¯æ‹‰å¸ƒæ‹‰å¤šã€‚å¥¹ä»¬ååœ¨æ²™æ»©ä¸Šï¼Œç‹—çš„å‰è…¿æŠ¬èµ·æ¥ï¼Œä¼¼ä¹åœ¨å’Œäººç±»å‡»æŒã€‚ä¸¤äººä¹‹é—´å……æ»¡äº†ä¿¡ä»»å’Œçˆ±ã€‚

# 2nd dialogue turn
response, history = model.chat(tokenizer, 'è¾“å‡º"å‡»æŒ"çš„æ£€æµ‹æ¡†', history=history)
print(response)
# <ref>å‡»æŒ</ref><box>(517,508),(589,611)</box>
image = tokenizer.draw_bbox_on_latest_picture(response, history)
if image:
  image.save('1.jpg')
else:
  print("no box")
```

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo_highfive.jpg" width="500"/>
<p>
<br>

## é‡åŒ– (Quantization)

### ç”¨æ³• (Usage)

å½“å‰æˆ‘ä»¬æä¾›äº†åŸºäº[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)çš„é‡åŒ–æ–¹æ¡ˆï¼Œå¹¶æä¾›äº†Qwen-VL-Chatçš„Int4é‡åŒ–ç‰ˆæœ¬Qwen-VL-Chat-Int4 [ç‚¹å‡»æ­¤å¤„](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4)ã€‚è¯¥æ¨¡å‹åœ¨æ•ˆæœè¯„æµ‹ä¸Šå‡ ä¹æ— æŸï¼Œå¹¶åœ¨æ˜¾å­˜å ç”¨å’Œæ¨ç†é€Ÿåº¦ä¸Šå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

ä¸‹æ–‡è¯´æ˜å¦‚ä½•ä½¿ç”¨è¯¥é‡åŒ–æ¨¡å‹ã€‚å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ æ»¡è¶³è¦æ±‚ï¼ˆå¦‚torch2.0åŠä»¥ä¸Šã€transformers 4.32.0åŠä»¥ä¸Šï¼Œç­‰ï¼‰å¹¶å®‰è£…æ‰€éœ€çš„ä»£ç åº“ï¼š

We provide a new solution based on [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), and release an Int4 quantized model for Qwen-VL-Chat, Qwen-VL-Chat-Int4 [Click here](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4), which achieves nearly lossless model effects but improved performance on both memory costs and inference speed.

Here we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements (e.g., torch 2.0 and above, transformers 4.32.0 and above, etc.) and install the required packages:

```bash
pip install optimum
git clone https://github.com/JustinLin610/AutoGPTQ.git & cd AutoGPTQ
pip install -v .
```

å¦‚é‡åˆ°å®‰è£… `auto-gptq` çš„é—®é¢˜ï¼Œå»ºè®®æ‚¨å‰å¾€å®˜æ–¹[repo](https://github.com/PanQiWei/AutoGPTQ) å¯»æ‰¾åˆé€‚çš„wheelã€‚

éšåä½ ä¾¿å¯ä»¥æŒ‰ç…§ä¸Šè¿°ç”¨æ³•ï¼Œè½»æ¾è°ƒç”¨é‡åŒ–æ¨¡å‹ï¼š

If you meet problems installing `auto-gptq`, we advise you to check out the official [repo](https://github.com/PanQiWei/AutoGPTQ) to find a wheel.

Then you can load the quantized model easily and run inference as same as usual:

```python
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
# Either a local path or an u[](https://)rl between <img></img> tags.
image_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'
response, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)
print(response)
```

### æ•ˆæœè¯„æµ‹ (Performance)

æˆ‘ä»¬åˆ—å‡ºä¸åŒç²¾åº¦ä¸‹æ¨¡å‹åœ¨è¯„æµ‹åŸºå‡† **[TouchStone](https://github.com/OFA-Sys/TouchStone)** ä¸Šçš„è¡¨ç°ï¼Œå¹¶å‘ç°é‡åŒ–æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾è‘—æ€§èƒ½æŸå¤±ã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

We illustrate the model performance of both BF16 and Int4 models on the benchmark **[TouchStone](https://github.com/OFA-Sys/TouchStone)**, and we find that the quantized model does not suffer from significant performance degradation. Results are shown below:

| Quantization | ZH.        | EN            |
| ------------ | :--------: | :-----------: | 
| BF16         | 401.2      |    645.2      |
| Int4         | 386.6      |    651.4      |

### æ¨ç†é€Ÿåº¦ (Inference Speed)

æˆ‘ä»¬æµ‹ç®—äº†åœ¨è¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼ˆå³258ä¸ªtokenï¼‰çš„æ¡ä»¶ä¸‹BF16å’ŒInt4çš„æ¨¡å‹ç”Ÿæˆ1792 (2048-258) å’Œ 7934 (8192-258) ä¸ªtokençš„å¹³å‡é€Ÿåº¦ã€‚

We measured the average inference speed (tokens/s) of generating 1792 (2048-258) and 7934 (8192-258) tokens with the context of an image (which takes 258 tokens) under BF16 precision and Int4 quantization, respectively.

| Quantization | Speed (2048 tokens) | Speed (8192 tokens) |
| ------------ | :-----------------: | :-----------------: |
| BF16         |        28.87        |        24.32        |
| Int4         |        37.79        |        34.34        |

æ¨ç†é€Ÿåº¦æµ‹ç®—æ˜¯åœ¨å•å¡ A100-SXM4-80G GPUä¸Šè¿è¡Œï¼Œä½¿ç”¨PyTorch 2.0.1åŠCUDA 11.4ã€‚

The profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.4.

### GPUæ˜¾å­˜å ç”¨ (GPU Memory Usage)

æˆ‘ä»¬è¿˜æµ‹ç®—äº†åœ¨ä¸€å¼ å›¾ç‰‡è¾“å…¥çš„æ¡ä»¶ä¸‹BF16å’ŒInt4æ¨¡å‹ç”Ÿæˆ1792 (2048-258) å’Œ 7934 (8192-258) ä¸ªtokenæ‰€éœ€æ˜¾å­˜ã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

We also profile the peak GPU memory usage for encoding 1792 (2048-258) tokens (including an image) as context (and generating single token) and generating 7934 (8192-258) tokens (with an image as context) under BF16 or Int4 quantization level, respectively. The results are shown below.

| Quantization | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |
| ------------ | :---------------------------------: | :-----------------------------------: |
| BF16         |               22.60GB               |                28.01GB                |
| Int4         |               11.82GB               |                17.23GB                |

ä¸Šè¿°é€Ÿåº¦å’Œæ˜¾å­˜æµ‹ç®—ä½¿ç”¨[æ­¤è„šæœ¬](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile_mm.py)å®Œæˆã€‚

The above speed and memory profiling are conducted using [this script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile_mm.py).
<br>

## è¯„æµ‹

æˆ‘ä»¬ä»ä¸¤ä¸ªè§’åº¦è¯„æµ‹äº†ä¸¤ä¸ªæ¨¡å‹çš„èƒ½åŠ›ï¼š

1. åœ¨**è‹±æ–‡æ ‡å‡† Benchmark** ä¸Šè¯„æµ‹æ¨¡å‹çš„åŸºç¡€ä»»åŠ¡èƒ½åŠ›ã€‚ç›®å‰è¯„æµ‹äº†å››å¤§ç±»å¤šæ¨¡æ€ä»»åŠ¡ï¼š
   
   - Zero-shot Caption: è¯„æµ‹æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å›¾ç‰‡æè¿°èƒ½åŠ›ï¼›
   - General VQA: è¯„æµ‹æ¨¡å‹çš„é€šç”¨é—®ç­”èƒ½åŠ›ï¼Œä¾‹å¦‚åˆ¤æ–­é¢˜ã€é¢œè‰²ã€ä¸ªæ•°ã€ç±»ç›®ç­‰é—®ç­”èƒ½åŠ›ï¼›
   - Text-based VQAï¼šè¯„æµ‹æ¨¡å‹å¯¹äºå›¾ç‰‡ä¸­æ–‡å­—ç›¸å…³çš„è¯†åˆ«/é—®ç­”èƒ½åŠ›ï¼Œä¾‹å¦‚æ–‡æ¡£é—®ç­”ã€å›¾è¡¨é—®ç­”ã€æ–‡å­—é—®ç­”ç­‰ï¼›
   - Referring Expression Compressionï¼šè¯„æµ‹æ¨¡å‹ç»™å®šç‰©ä½“æè¿°ç”»æ£€æµ‹æ¡†çš„èƒ½åŠ›ï¼›
2. **è¯•é‡‘çŸ³ (TouchStone)**ï¼šä¸ºäº†è¯„æµ‹æ¨¡å‹æ•´ä½“çš„å›¾æ–‡å¯¹è¯èƒ½åŠ›å’Œäººç±»å¯¹é½æ°´å¹³ã€‚æˆ‘ä»¬ä¸ºæ­¤æ„å»ºäº†ä¸€ä¸ªåŸºäº GPT4 æ‰“åˆ†æ¥è¯„æµ‹ LVLM æ¨¡å‹çš„ Benchmarkï¼šTouchStoneã€‚åœ¨ TouchStone-v0.1 ä¸­ï¼š
   
   - è¯„æµ‹åŸºå‡†æ€»è®¡æ¶µç›– 300+å¼ å›¾ç‰‡ã€800+é“é¢˜ç›®ã€27ä¸ªç±»åˆ«ã€‚åŒ…æ‹¬åŸºç¡€å±æ€§é—®ç­”ã€äººç‰©åœ°æ ‡é—®ç­”ã€å½±è§†ä½œå“é—®ç­”ã€è§†è§‰æ¨ç†ã€åäº‹å®æ¨ç†ã€è¯—æ­Œåˆ›ä½œã€æ•…äº‹å†™ä½œï¼Œå•†å“æ¯”è¾ƒã€å›¾ç‰‡è§£é¢˜ç­‰**å°½å¯èƒ½å¹¿æ³›çš„ç±»åˆ«**ã€‚
   - ä¸ºäº†å¼¥è¡¥ç›®å‰ GPT4 æ— æ³•ç›´æ¥è¯»å–å›¾ç‰‡çš„ç¼ºé™·ï¼Œæˆ‘ä»¬ç»™æ‰€æœ‰çš„å¸¦è¯„æµ‹å›¾ç‰‡æä¾›äº†**äººå·¥æ ‡æ³¨çš„å……åˆ†è¯¦ç»†æè¿°**ï¼Œå¹¶ä¸”å°†å›¾ç‰‡çš„è¯¦ç»†æè¿°ã€é—®é¢˜å’Œæ¨¡å‹çš„è¾“å‡ºç»“æœä¸€èµ·äº¤ç»™ GPT4 æ‰“åˆ†ã€‚
   - è¯„æµ‹åŒæ—¶åŒ…å«è‹±æ–‡ç‰ˆæœ¬å’Œä¸­æ–‡ç‰ˆæœ¬ã€‚

è¯„æµ‹ç»“æœå¦‚ä¸‹ï¼š

We evaluated the model's ability from two perspectives:

1. **Standard Benchmarks**: We evaluate the model's basic task capabilities on four major categories of multimodal tasks:
   
   - Zero-shot Caption: Evaluate model's zero-shot image captioning ability on unseen datasets;
   - General VQA: Evaluate the general question-answering ability of pictures, such as the judgment, color, number, category, etc;
   - Text-based VQA: Evaluate the model's ability to recognize text in pictures, such as document QA, chart QA, etc;
   - Referring Expression Comprehension: Evaluate the ability to localize a target object in an image described by a referring expression.
2. **TouchStone**: To evaluate the overall text-image dialogue capability and alignment level with humans, we have constructed a benchmark called TouchStone, which is based on scoring with GPT4 to evaluate the LVLM model.
   
   - The TouchStone benchmark covers a total of 300+ images, 800+ questions, and 27 categories. Such as attribute-based Q&A, celebrity recognition, writing poetry, summarizing multiple images, product comparison, math problem solving, etc;
   - In order to break the current limitation of GPT4 in terms of direct image input, TouchStone provides fine-grained image annotations by human labeling. These detailed annotations, along with the questions and the model's output, are then presented to GPT4 for scoring.
   - The benchmark includes both English and Chinese versions.

The results of the evaluation are as follows:

Qwen-VL outperforms current SOTA generalist models on multiple VL tasks and has a more comprehensive coverage in terms of capability range.

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/radar.png" width="600"/>
<p>

### é›¶æ ·æœ¬å›¾åƒæè¿° & é€šç”¨è§†è§‰é—®ç­” (Zero-shot Captioning & General VQA)

<table>
<thead>
  <tr>
    <th rowspan="2">Model type</th>
    <th rowspan="2">Model</th>
    <th colspan="2">Zero-shot Captioning</th>
    <th colspan="5">General VQA</th>
  </tr>
  <tr>
    <th>NoCaps</th>
    <th>Flickr30K</th>
    <th>VQAv2<sup>dev</sup></th>
    <th>OK-VQA</th>
    <th>GQA</th>
    <th>SciQA-Img<br>(0-shot)</th>
    <th>VizWiz<br>(0-shot)</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="10">Generalist<br>Models</td>
    <td>Flamingo-9B</td>
    <td>-</td>
    <td>61.5</td>
    <td>51.8</td>
    <td>44.7</td>
    <td>-</td>
    <td>-</td>
    <td>28.8</td>
  </tr>
  <tr>
    <td>Flamingo-80B</td>
    <td>-</td>
    <td>67.2</td>
    <td>56.3</td>
    <td>50.6</td>
    <td>-</td>
    <td>-</td>
    <td>31.6</td>
  </tr>
  <tr>
    <td>Unified-IO-XL</td>
    <td>100.0</td>
    <td>-</td>
    <td>77.9</td>
    <td>54.0</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Kosmos-1</td>
    <td>-</td>
    <td>67.1</td>
    <td>51.0</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>29.2</td>
  </tr>
  <tr>
    <td>Kosmos-2</td>
    <td>-</td>
    <td>66.7</td>
    <td>45.6</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>BLIP-2 (Vicuna-13B)</td>
    <td>103.9</td>
    <td>71.6</td>
    <td>65.0</td>
    <td>45.9</td>
    <td>32.3</td>
    <td>61.0</td>
    <td>19.6</td>
  </tr>
  <tr>
    <td>InstructBLIP (Vicuna-13B)</td>
    <td><strong>121.9</strong></td>
    <td>82.8</td>
    <td>-</td>
    <td>-</td>
    <td>49.5</td>
    <td>63.1</td>
    <td>33.4</td>
  </tr>
  <tr>
    <td>Shikra (Vicuna-13B)</td>
    <td>-</td>
    <td>73.9</td>
    <td>77.36</td>
    <td>47.16</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td><strong>Qwen-VL (Qwen-7B)</strong></td>
    <td>121.4</td>
    <td><b>85.8</b></td>
    <td><b>78.8</b></td>
    <td><b>58.6</b></td>
    <td><b>59.3</b></td>
    <td>67.1</td>
    <td>35.2</td>
  </tr>
  <!-- <tr>
    <td>Qwen-VL (4-shot)</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>63.6</td>
    <td>-</td>
    <td>-</td>
    <td>39.1</td>
  </tr> -->
  <tr>
    <td>Qwen-VL-Chat</td>
    <td>120.2</td>
    <td>81.0</td>
    <td>78.2</td>
    <td>56.6</td>
    <td>57.5</td>
    <td><b>68.2</b></td>
    <td><b>38.9</b></td>
  </tr>
  <!-- <tr>
    <td>Qwen-VL-Chat (4-shot)</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>60.6</td>
    <td>-</td>
    <td>-</td>
    <td>44.45</td>
  </tr> -->
  <tr>
    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>
    <td>-</td>
    <td>127.0<br>(PALI-17B)</td>
    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>
    <td>86.1<br>(PALI-X<br>-55B)</td>
    <td>66.1<br>(PALI-X<br>-55B)</td>
    <td>72.1<br>(CFR)</td>
    <td>92.53<br>(LLaVa+<br>GPT-4)</td>
    <td>70.9<br>(PALI-X<br>-55B)</td>
  </tr>
</tbody>
</table>

- åœ¨ Zero-shot Caption ä¸­ï¼ŒQwen-VL åœ¨ Flickr30K æ•°æ®é›†ä¸Šå–å¾—äº† **SOTA** çš„ç»“æœï¼Œå¹¶åœ¨ Nocaps æ•°æ®é›†ä¸Šå–å¾—äº†å’Œ InstructBlip å¯ç«äº‰çš„ç»“æœã€‚
- åœ¨ General VQA ä¸­ï¼ŒQwen-VL å–å¾—äº† LVLM æ¨¡å‹åŒç­‰é‡çº§å’Œè®¾å®šä¸‹ **SOTA** çš„ç»“æœã€‚
- For zero-shot image captioning, Qwen-VL achieves the **SOTA** on Flickr30K and competitive results on Nocaps with InstructBlip.
- For general VQA, Qwen-VL achieves the **SOTA** under the same generalist LVLM scale settings.

### æ–‡æœ¬å¯¼å‘çš„è§†è§‰é—®ç­” (Text-oriented VQA)

<table>
<thead>
  <tr>
    <th>Model type</th>
    <th>Model</th>
    <th>TextVQA</th>
    <th>DocVQA</th>
    <th>ChartQA</th>
    <th>AI2D</th>
    <th>OCR-VQA</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="5">Generalist Models</td>
    <td>BLIP-2 (Vicuna-13B)</td>
    <td>42.4</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>InstructBLIP (Vicuna-13B)</td>
    <td>50.7</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>mPLUG-DocOwl (LLaMA-7B)</td>
    <td>52.6</td>
    <td>62.2</td>
    <td>57.4</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Pic2Struct-Large (1.3B)</td>
    <td>-</td>
    <td><b>76.6</b></td>
    <td>58.6</td>
    <td>42.1</td>
    <td>71.3</td>
  </tr>
  <tr>
    <td>Qwen-VL (Qwen-7B)</td>
    <td><b>63.8</b></td>
    <td>65.1</td>
    <td><b>65.7</b></td>
    <td><b>62.3</b></td>
    <td><b>75.7</b></td>
  </tr>
  <tr>
    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>
    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>
    <td>71.44</td>
    <td>80.0</td>
    <td>70.0</td>
    <td>81.2</td>
    <td>75.0</td>
  </tr>
</tbody>
</table>

- åœ¨æ–‡å­—ç›¸å…³çš„è¯†åˆ«/é—®ç­”è¯„æµ‹ä¸Šï¼Œå–å¾—äº†å½“å‰è§„æ¨¡ä¸‹é€šç”¨ LVLM è¾¾åˆ°çš„æœ€å¥½ç»“æœã€‚
- åˆ†è¾¨ç‡å¯¹ä¸Šè¿°æŸå‡ ä¸ªè¯„æµ‹éå¸¸é‡è¦ï¼Œå¤§éƒ¨åˆ† 224 åˆ†è¾¨ç‡çš„å¼€æº LVLM æ¨¡å‹æ— æ³•å®Œæˆä»¥ä¸Šè¯„æµ‹ï¼Œæˆ–åªèƒ½é€šè¿‡åˆ‡å›¾çš„æ–¹å¼è§£å†³ã€‚Qwen-VL å°†åˆ†è¾¨ç‡æå‡åˆ° 448ï¼Œå¯ä»¥ç›´æ¥ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œä»¥ä¸Šè¯„æµ‹ã€‚Qwen-VL åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šç”šè‡³è¶…è¿‡äº† 1024 åˆ†è¾¨ç‡çš„ Pic2Struct-Large æ¨¡å‹ã€‚
- In text-related recognition/QA evaluation, Qwen-VL achieves the SOTA under the generalist LVLM scale settings.
- Resolution is important for several above evaluations. While most open-source LVLM models with 224 resolution are incapable of these evaluations or can only solve these by cutting images, Qwen-VL scales the resolution to 448 so that it can be evaluated end-to-end. Qwen-VL even outperforms Pic2Struct-Large models of 1024 resolution on some tasks.

### ç»†ç²’åº¦è§†è§‰å®šä½ (Referring Expression Comprehension)

<table>
<thead>
  <tr>
    <th rowspan="2">Model type</th>
    <th rowspan="2">Model</th>
    <th colspan="3">RefCOCO</th>
    <th colspan="3">RefCOCO+</th>
    <th colspan="2">RefCOCOg</th>
    <th>GRIT</th>
  </tr>
  <tr>
    <th>val</th>
    <th>test-A</th>
    <th>test-B</th>
    <th>val</th>
    <th>test-A</th>
    <th>test-B</th>
    <th>val-u</th>
    <th>test-u</th>
    <th>refexp</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="8">Generalist Models</td>
    <td>GPV-2</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>51.50</td>
  </tr>
  <tr>
    <td>OFA-L*</td>
    <td>79.96</td>
    <td>83.67</td>
    <td>76.39</td>
    <td>68.29</td>
    <td>76.00</td>
    <td>61.75</td>
    <td>67.57</td>
    <td>67.58</td>
    <td>61.70</td>
  </tr>
  <tr>
    <td>Unified-IO</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td><b>78.61</b></td>
  </tr>
  <tr>
    <td>VisionLLM-H</td>
    <td></td>
    <td>86.70</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Shikra-7B</td>
    <td>87.01</td>
    <td>90.61</td>
    <td>80.24 </td>
    <td>81.60</td>
    <td>87.36</td>
    <td>72.12</td>
    <td>82.27</td>
    <td>82.19</td>
    <td>69.34</td>
  </tr>
  <tr>
    <td>Shikra-13B</td>
    <td>87.83 </td>
    <td>91.11</td>
    <td>81.81</td>
    <td>82.89</td>
    <td>87.79</td>
    <td>74.41</td>
    <td>82.64</td>
    <td>83.16</td>
    <td>69.03</td>
  </tr>
  <tr>
    <td>Qwen-VL-7B</td>
    <td><b>89.36</b></td>
    <td>92.26</td>
    <td><b>85.34</b></td>
    <td><b>83.12</b></td>
    <td>88.25</td>
    <td><b>77.21</b></td>
    <td>85.58</td>
    <td>85.48</td>
    <td>78.22</td>
  </tr>
  <tr>
    <td>Qwen-VL-7B-Chat</td>
    <td>88.55</td>
    <td><b>92.27</b></td>
    <td>84.51</td>
    <td>82.82</td>
    <td><b>88.59</b></td>
    <td>76.79</td>
    <td><b>85.96</b></td>
    <td><b>86.32</b></td>
    <td>-</td>
  <tr>
    <td rowspan="3">Specialist SOTAs<br>(Specialist/Finetuned)</td>
    <td>G-DINO-L</td>
    <td>90.56&nbsp;&nbsp;</td>
    <td>93.19</td>
    <td>88.24</td>
    <td>82.75</td>
    <td>88.95</td>
    <td>75.92</td>
    <td>86.13</td>
    <td>87.02</td>
    <td>-</td>
  </tr>
  <tr>
    <td>UNINEXT-H</td>
    <td>92.64 </td>
    <td>94.33</td>
    <td>91.46</td>
    <td>85.24</td>
    <td>89.63</td>
    <td>79.79</td>
    <td>88.73</td>
    <td>89.37</td>
    <td>-</td>
  </tr>
  <tr>
    <td>ONE-PEACE</td>
    <td>92.58 </td>
    <td>94.18</td>
    <td>89.26</td>
    <td>88.77</td>
    <td>92.21</td>
    <td>83.23</td>
    <td>89.22</td>
    <td>89.27</td>
    <td>-</td>
  </tr>
</tbody>
</table>

- åœ¨å®šä½ä»»åŠ¡ä¸Šï¼ŒQwen-VL å…¨é¢è¶…è¿‡ Shikra-13Bï¼Œå–å¾—äº†ç›®å‰ Generalist LVLM æ¨¡å‹ä¸Šåœ¨ Refcoco ä¸Šçš„ **SOTA**ã€‚
- Qwen-VL å¹¶æ²¡æœ‰åœ¨ä»»ä½•ä¸­æ–‡å®šä½æ•°æ®ä¸Šè®­ç»ƒè¿‡ï¼Œä½†é€šè¿‡ä¸­æ–‡ Caption æ•°æ®å’Œ è‹±æ–‡ Grounding æ•°æ®çš„è®­ç»ƒï¼Œå¯ä»¥ Zero-shot æ³›åŒ–å‡ºä¸­æ–‡ Grounding èƒ½åŠ›ã€‚

æˆ‘ä»¬æä¾›äº†ä»¥ä¸Š**æ‰€æœ‰**è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚è¯·é˜…è¯» [eval/EVALUATION.md](eval/EVALUATION.md) äº†è§£æ›´å¤šä¿¡æ¯ã€‚

- Qwen-VL achieves the **SOTA** in all above referring expression comprehension benchmarks.
- Qwen-VL has not been trained on any Chinese grounding data, but it can still generalize to the Chinese Grounding tasks in a zero-shot way by training Chinese Caption data and English Grounding data.

We provide all of the above evaluation scripts for reproducing our experimental results. Please read [eval/EVALUATION.md](eval/EVALUATION.md) for more information.

### é—²èŠèƒ½åŠ›æµ‹è¯„ (Chat Evaluation)

TouchStone æ˜¯ä¸€ä¸ªåŸºäº GPT4 æ‰“åˆ†æ¥è¯„æµ‹ LVLM æ¨¡å‹çš„å›¾æ–‡å¯¹è¯èƒ½åŠ›å’Œäººç±»å¯¹é½æ°´å¹³çš„åŸºå‡†ã€‚å®ƒæ¶µç›–äº† 300+å¼ å›¾ç‰‡ã€800+é“é¢˜ç›®ã€27ä¸ªç±»åˆ«ï¼ŒåŒ…æ‹¬åŸºç¡€å±æ€§ã€äººç‰©åœ°æ ‡ã€è§†è§‰æ¨ç†ã€è¯—æ­Œåˆ›ä½œã€æ•…äº‹å†™ä½œã€å•†å“æ¯”è¾ƒã€å›¾ç‰‡è§£é¢˜ç­‰**å°½å¯èƒ½å¹¿æ³›çš„ç±»åˆ«**ã€‚å…³äº TouchStone çš„è¯¦ç»†ä»‹ç»ï¼Œè¯·å‚è€ƒ[touchstone/README_CN.md](touchstone/README_CN.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚

TouchStone is a benchmark based on scoring with GPT4 to evaluate the abilities of the LVLM model on text-image dialogue and alignment levels with humans. It covers a total of 300+ images, 800+ questions, and 27 categories, such as attribute-based Q&A, celebrity recognition, writing poetry, summarizing multiple images, product comparison, math problem solving, etc. Please read [touchstone/README_CN.md](touchstone/README.md) for more information.

#### è‹±è¯­ (English)

| Model         | Score |
|---------------|-------|
| PandaGPT      | 488.5 |
| MiniGPT4      | 531.7 |
| InstructBLIP  | 552.4 |
| LLaMA-AdapterV2 | 590.1 |
| mPLUG-Owl     | 605.4 |
| LLaVA         | 602.7 |
| Qwen-VL-Chat   | 645.2 |

#### ä¸­æ–‡ (Chinese)

| Model         | Score |
|---------------|-------|
| VisualGLM     | 247.1 |
| Qwen-VL-Chat   | 401.2 |

Qwen-VL-Chat æ¨¡å‹åœ¨ä¸­è‹±æ–‡çš„å¯¹é½è¯„æµ‹ä¸­å‡å–å¾—å½“å‰ LVLM æ¨¡å‹ä¸‹çš„æœ€å¥½ç»“æœã€‚

Qwen-VL-Chat has achieved the best results in both Chinese and English alignment evaluation.
<br>

## å¸¸è§é—®é¢˜ (FAQ)

å¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜… [FAQ](https://github.com/QwenLM/Qwen-VL/blob/master/FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚

If you meet problems, please refer to [FAQ](https://github.com/QwenLM/Qwen-VL/blob/master/FAQ.md) and the issues first to search a solution before you launch a new issue.
<br>

## ä½¿ç”¨åè®® (License Agreement)

ç ”ç©¶äººå‘˜ä¸å¼€å‘è€…å¯ä½¿ç”¨Qwen-VLå’ŒQwen-VL-Chatæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚æˆ‘ä»¬åŒæ ·å…è®¸å•†ä¸šä½¿ç”¨ï¼Œå…·ä½“ç»†èŠ‚è¯·æŸ¥çœ‹[LICENSE](https://github.com/QwenLM/Qwen-VL/blob/master/LICENSE)ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·å¡«å†™[é—®å·](https://dashscope.console.aliyun.com/openModelApply/qianwen)ç”³è¯·ã€‚

Researchers and developers are free to use the codes and model weights of both Qwen-VL and Qwen-VL-Chat. We also allow their commercial use. Check our license at [LICENSE](LICENSE) for more details.
<br>

## å¼•ç”¨ (Citation)

å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„è®ºæ–‡å’Œä»£ç å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘:star: å’Œå¼•ç”¨ :pencil: :)

If you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)

```BibTeX
@article{Qwen-VL,
  title={Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}
```
<br>

## è”ç³»æˆ‘ä»¬ (Contact Us)

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œè¯·é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚

If you are interested to leave a message to either our research team or product team, feel free to send an email to qianwen_opensource@alibabacloud.com.

```

```

