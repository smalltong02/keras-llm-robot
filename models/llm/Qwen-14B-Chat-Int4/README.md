---
language:
- zh
- en
tags:
- qwen
pipeline_tag: text-generation
inference: false
---

# Qwen-14B-Chat-Int4

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"/>
<p>
<br>

<p align="center">
        ğŸ¤— <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/organization/qwen">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="https://arxiv.org/abs/2309.16609">Paper</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href="https://modelscope.cn/studios/qwen/Qwen-14B-Chat-Demo/summary">Demo</a>
<br>
<a href="assets/wechat.png">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp ï½œ  &nbsp&nbsp<a href="https://dashscope.aliyun.com">API</a> 
</p>
<br>

## ä»‹ç»ï¼ˆIntroductionï¼‰

**é€šä¹‰åƒé—®-14Bï¼ˆQwen-14Bï¼‰**æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„é€šä¹‰åƒé—®å¤§æ¨¡å‹ç³»åˆ—çš„140äº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹ã€‚Qwen-14Bæ˜¯åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹, åœ¨è¶…å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¾—åˆ°ã€‚é¢„è®­ç»ƒæ•°æ®ç±»å‹å¤šæ ·ï¼Œè¦†ç›–å¹¿æ³›ï¼ŒåŒ…æ‹¬å¤§é‡ç½‘ç»œæ–‡æœ¬ã€ä¸“ä¸šä¹¦ç±ã€ä»£ç ç­‰ã€‚åŒæ—¶ï¼Œåœ¨Qwen-14Bçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½æœºåˆ¶æ‰“é€ äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„AIåŠ©æ‰‹Qwen-14B-Chatã€‚æœ¬ä»“åº“ä¸ºQwen-14B-Chatçš„Int4é‡åŒ–æ¨¡å‹çš„ä»“åº“ã€‚

å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äºé€šä¹‰åƒé—®-14Bå¼€æºæ¨¡å‹çš„ç»†èŠ‚ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨å‚é˜…[GitHubä»£ç åº“](https://github.com/QwenLM/Qwen)ã€‚

**Qwen-14B** is the 14B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-14B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-14B, we release Qwen-14B-Chat, a large-model-based AI assistant, which is trained with alignment techniques. This repository is the one for the Int4 quantized model of Qwen-14B-Chat.

For more details about the open-source model of Qwen-14B, please refer to the [GitHub](https://github.com/QwenLM/Qwen) code repository.
<br>


## è¦æ±‚ï¼ˆRequirementsï¼‰

* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
* pytorch 2.0åŠä»¥ä¸Šç‰ˆæœ¬
* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
* python 3.8 and above
* pytorch 2.0 and above, 2.0 and above are recommended
* CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)
<br>


## ä¾èµ–é¡¹ï¼ˆDependencyï¼‰

è¿è¡ŒQwen-14B-Chat-Int4ï¼Œè¯·ç¡®ä¿æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œå†æ‰§è¡Œä»¥ä¸‹pipå‘½ä»¤å®‰è£…ä¾èµ–åº“ã€‚å¦‚å®‰è£…`auto-gptq`é‡åˆ°é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨åˆ°å®˜æ–¹[repo](https://github.com/PanQiWei/AutoGPTQ)æœç´¢åˆé€‚çš„é¢„ç¼–è¯‘wheelã€‚

To run Qwen-14B-Chat-Int4, please make sure you meet the above requirements, and then execute the following pip commands to install the dependent libraries. If you meet problems installing `auto-gptq`, we advise you to check out the official [repo](https://github.com/PanQiWei/AutoGPTQ) to find a pre-build wheel.

```bash
pip install transformers==4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed
pip install auto-gptq optimum
```

å¦å¤–ï¼Œæ¨èå®‰è£…`flash-attention`åº“ï¼ˆ**å½“å‰å·²æ”¯æŒflash attention 2**ï¼‰ï¼Œä»¥å®ç°æ›´é«˜çš„æ•ˆç‡å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨ã€‚

In addition, it is recommended to install the `flash-attention` library (**we support flash attention 2 now.**) for higher efficiency and lower memory usage.

```bash
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# ä¸‹æ–¹å®‰è£…å¯é€‰ï¼Œå®‰è£…å¯èƒ½æ¯”è¾ƒç¼“æ…¢ã€‚
# pip install csrc/layer_norm
# pip install csrc/rotary
```
<br>



## å¿«é€Ÿä½¿ç”¨ï¼ˆQuickstartï¼‰

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªä½¿ç”¨Qwen-14B-Chat-Int4æ¨¡å‹çš„æ ·ä¾‹ï¼š

We show an example of how to use Qwen-14B-Chat-Int4 in the following code:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# Note: The default behavior now has injection attack prevention off.
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-14B-Chat-Int4", trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-14B-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚
```

å…³äºæ›´å¤šçš„ä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„[GitHub repo](https://github.com/QwenLM/Qwen)è·å–æ›´å¤šä¿¡æ¯ã€‚

For more information, please refer to our [GitHub repo](https://github.com/QwenLM/Qwen) for more information.
<br>



## é‡åŒ– (Quantization)

### æ•ˆæœè¯„æµ‹

æˆ‘ä»¬å¯¹BF16ï¼ŒInt8å’ŒInt4æ¨¡å‹åœ¨åŸºå‡†è¯„æµ‹ä¸Šåšäº†æµ‹è¯•ï¼ˆä½¿ç”¨zero-shotè®¾ç½®ï¼‰ï¼Œå‘ç°é‡åŒ–æ¨¡å‹æ•ˆæœæŸå¤±è¾ƒå°ï¼Œç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

We illustrate the zero-shot performance of both BF16, Int8 and Int4 models on the benchmark, and we find that the quantized model does not suffer from significant performance degradation. Results are shown below:

| Quantization | MMLU | CEval (val) | GSM8K | Humaneval |
|--------------|:----:|:-----------:|:-----:|:---------:|
| BF16         | 64.6 |    69.8     | 60.1  |   43.9    |
| Int8         | 63.6 |    68.6     | 60.0	|   48.2    |
| Int4         | 63.3 |    69.0     | 59.8  |   45.7    |

### æ¨ç†é€Ÿåº¦ (Inference Speed)

æˆ‘ä»¬æµ‹ç®—äº†ä¸åŒç²¾åº¦æ¨¡å‹ä»¥åŠä¸åŒFlashAttnåº“ç‰ˆæœ¬ä¸‹æ¨¡å‹ç”Ÿæˆ2048å’Œ8192ä¸ªtokençš„å¹³å‡æ¨ç†é€Ÿåº¦ã€‚å¦‚å›¾æ‰€ç¤ºï¼š

We measured the average inference speed of generating 2048 and 8192 tokens with different quantization levels and versions of flash-attention, respectively.

|  Quantization | FlashAttn | Speed (2048 tokens) | Speed (8192 tokens) |
| ------------- | :-------: | :------------------:| :------------------:|
|      BF16     |   v2      | 32.88               | 24.87               |
|      Int8     |   v2      | 29.28               | 24.22               |
|      Int4     |   v2      | 38.72               | 27.33               |
|      BF16     |   v1      | 32.76               | 28.89               |
|      Int8     |   v1      | 28.31               | 23.87               |
|      Int4     |   v1      | 37.81               | 26.46               |
|      BF16     |  Disabled | 29.32               | 22.91               |
|      Int8     |  Disabled | 31.12               | 24.60               |
|      Int4     |  Disabled | 37.65               | 26.00               |

å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®°å½•åœ¨é•¿åº¦ä¸º1çš„ä¸Šä¸‹æ–‡çš„æ¡ä»¶ä¸‹ç”Ÿæˆ8192ä¸ªtokençš„æ€§èƒ½ã€‚è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œä½¿ç”¨PyTorch 2.0.1å’ŒCUDA 11.8ã€‚æ¨ç†é€Ÿåº¦æ˜¯ç”Ÿæˆ8192ä¸ªtokençš„é€Ÿåº¦å‡å€¼ã€‚

In detail, the setting of profiling is generating 8192 new tokens with 1 context token. The profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.8. The inference speed is averaged over the generated 8192 tokens.

æ³¨æ„ï¼šä»¥ä¸ŠInt4/Int8æ¨¡å‹ç”Ÿæˆé€Ÿåº¦ä½¿ç”¨autogptqåº“ç»™å‡ºï¼Œå½“å‰``AutoModelForCausalLM.from_pretrained``è½½å…¥çš„æ¨¡å‹ç”Ÿæˆé€Ÿåº¦ä¼šæ…¢å¤§çº¦20%ã€‚æˆ‘ä»¬å·²ç»å°†è¯¥é—®é¢˜æ±‡æŠ¥ç»™HuggingFaceå›¢é˜Ÿï¼Œè‹¥æœ‰è§£å†³æ–¹æ¡ˆå°†å³æ—¶æ›´æ–°ã€‚

Note: The generation speed of the Int4/Int8 models mentioned above is provided by the autogptq library. The current speed of the model loaded using "AutoModelForCausalLM.from_pretrained" will be approximately 20% slower. We have reported this issue to the HuggingFace team and will update it promptly if a solution is available.

### æ˜¾å­˜ä½¿ç”¨ (GPU Memory Usage)

æˆ‘ä»¬è¿˜æµ‹ç®—äº†ä¸åŒæ¨¡å‹ç²¾åº¦ç¼–ç 2048ä¸ªtokenåŠç”Ÿæˆ8192ä¸ªtokençš„å³°å€¼æ˜¾å­˜å ç”¨æƒ…å†µã€‚ï¼ˆæ˜¾å­˜æ¶ˆè€—åœ¨æ˜¯å¦ä½¿ç”¨FlashAttnçš„æƒ…å†µä¸‹å‡ç±»ä¼¼ã€‚ï¼‰ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

We also profile the peak GPU memory usage for encoding 2048 tokens as context (and generating single token) and generating 8192 tokens (with single token as context) under different quantization levels, respectively. ï¼ˆThe GPU memory usage is similar when using flash-attention or not.ï¼‰The results are shown below.

| Quantization Level | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |
| ------------------ | :---------------------------------: | :-----------------------------------: |
| BF16               | 30.15GB                             | 38.94GB                               |
| Int8               | 18.81GB                             | 27.54GB                               |
| Int4               | 13.01GB                             | 21.79GB                               |

ä¸Šè¿°æ€§èƒ½æµ‹ç®—ä½¿ç”¨[æ­¤è„šæœ¬](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py)å®Œæˆã€‚

The above speed and memory profiling are conducted using [this script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py).
<br>

## Tokenizer

> æ³¨ï¼šä½œä¸ºæœ¯è¯­çš„â€œtokenizationâ€åœ¨ä¸­æ–‡ä¸­å°šæ— å…±è¯†çš„æ¦‚å¿µå¯¹åº”ï¼Œæœ¬æ–‡æ¡£é‡‡ç”¨è‹±æ–‡è¡¨è¾¾ä»¥åˆ©è¯´æ˜ã€‚

åŸºäºtiktokençš„åˆ†è¯å™¨æœ‰åˆ«äºå…¶ä»–åˆ†è¯å™¨ï¼Œæ¯”å¦‚sentencepieceåˆ†è¯å™¨ã€‚å°¤å…¶åœ¨å¾®è°ƒé˜¶æ®µï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ç‰¹æ®Štokençš„ä½¿ç”¨ã€‚å…³äºtokenizerçš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠå¾®è°ƒæ—¶æ¶‰åŠçš„ç›¸å…³ä½¿ç”¨ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://github.com/QwenLM/Qwen/blob/main/tokenization_note_zh.md)ã€‚

Our tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the [documentation](https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md).
<br>



## æ¨¡å‹ç»†èŠ‚ï¼ˆModelï¼‰

ä¸Qwen-14Bé¢„è®­ç»ƒæ¨¡å‹ç›¸åŒï¼ŒQwen-14B-Chatæ¨¡å‹è§„æ¨¡åŸºæœ¬æƒ…å†µå¦‚ä¸‹æ‰€ç¤º

The details of the model architecture of Qwen-14B-Chat are listed as follows

| Hyperparameter  | Value  |
|:----------------|:------:|
| n_layers        |   40   |
| n_heads         |   40   |
| d_model         |  5120  |
| vocab size      | 151851 |
| sequence length |  2048  |

åœ¨ä½ç½®ç¼–ç ã€FFNæ¿€æ´»å‡½æ•°å’Œnormalizationçš„å®ç°æ–¹å¼ä¸Šï¼Œæˆ‘ä»¬ä¹Ÿé‡‡ç”¨äº†ç›®å‰æœ€æµè¡Œçš„åšæ³•ï¼Œ
å³RoPEç›¸å¯¹ä½ç½®ç¼–ç ã€SwiGLUæ¿€æ´»å‡½æ•°ã€RMSNormï¼ˆå¯é€‰å®‰è£…flash-attentionåŠ é€Ÿï¼‰ã€‚

åœ¨åˆ†è¯å™¨æ–¹é¢ï¼Œç›¸æ¯”ç›®å‰ä¸»æµå¼€æºæ¨¡å‹ä»¥ä¸­è‹±è¯è¡¨ä¸ºä¸»ï¼ŒQwen-14B-Chatä½¿ç”¨äº†çº¦15ä¸‡tokenå¤§å°çš„è¯è¡¨ã€‚
è¯¥è¯è¡¨åœ¨GPT-4ä½¿ç”¨çš„BPEè¯è¡¨`cl100k_base`åŸºç¡€ä¸Šï¼Œå¯¹ä¸­æ–‡ã€å¤šè¯­è¨€è¿›è¡Œäº†ä¼˜åŒ–ï¼Œåœ¨å¯¹ä¸­ã€è‹±ã€ä»£ç æ•°æ®çš„é«˜æ•ˆç¼–è§£ç çš„åŸºç¡€ä¸Šï¼Œå¯¹éƒ¨åˆ†å¤šè¯­è¨€æ›´åŠ å‹å¥½ï¼Œæ–¹ä¾¿ç”¨æˆ·åœ¨ä¸æ‰©å±•è¯è¡¨çš„æƒ…å†µä¸‹å¯¹éƒ¨åˆ†è¯­ç§è¿›è¡Œèƒ½åŠ›å¢å¼ºã€‚
è¯è¡¨å¯¹æ•°å­—æŒ‰å•ä¸ªæ•°å­—ä½åˆ‡åˆ†ã€‚è°ƒç”¨è¾ƒä¸ºé«˜æ•ˆçš„[tiktokenåˆ†è¯åº“](https://github.com/openai/tiktoken)è¿›è¡Œåˆ†è¯ã€‚

For position encoding, FFN activation function, and normalization calculation methods, we adopt the prevalent practices, i.e., RoPE relative position encoding, SwiGLU for activation function, and RMSNorm for normalization (optional installation of flash-attention for acceleration).

For tokenization, compared to the current mainstream open-source models based on Chinese and English vocabularies, Qwen-14B-Chat uses a vocabulary of over 150K tokens.
It first considers efficient encoding of Chinese, English, and code data, and is also more friendly to multilingual languages, enabling users to directly enhance the capability of some languages without expanding the vocabulary.
It segments numbers by single digit, and calls the [tiktoken](https://github.com/openai/tiktoken) tokenizer library for efficient tokenization.
<br>



## è¯„æµ‹æ•ˆæœï¼ˆEvaluationï¼‰

å¯¹äºQwen-14B-Chatæ¨¡å‹ï¼Œæˆ‘ä»¬åŒæ ·è¯„æµ‹äº†å¸¸è§„çš„ä¸­æ–‡ç†è§£ï¼ˆC-Evalï¼‰ã€è‹±æ–‡ç†è§£ï¼ˆMMLUï¼‰ã€ä»£ç ï¼ˆHumanEvalï¼‰å’Œæ•°å­¦ï¼ˆGSM8Kï¼‰ç­‰æƒå¨ä»»åŠ¡ï¼ŒåŒæ—¶åŒ…å«äº†é•¿åºåˆ—ä»»åŠ¡çš„è¯„æµ‹ç»“æœã€‚ç”±äºQwen-14B-Chatæ¨¡å‹ç»è¿‡å¯¹é½åï¼Œæ¿€å‘äº†è¾ƒå¼ºçš„å¤–éƒ¨ç³»ç»Ÿè°ƒç”¨èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å·¥å…·ä½¿ç”¨èƒ½åŠ›æ–¹é¢çš„è¯„æµ‹ã€‚

æç¤ºï¼šç”±äºç¡¬ä»¶å’Œæ¡†æ¶é€ æˆçš„èˆå…¥è¯¯å·®ï¼Œå¤ç°ç»“æœå¦‚æœ‰æ³¢åŠ¨å±äºæ­£å¸¸ç°è±¡ã€‚

For Qwen-14B-Chat, we also evaluate the model on C-Eval, MMLU, HumanEval, GSM8K, etc., as well as the benchmark evaluation for long-context understanding, and tool usage.

Note: Due to rounding errors caused by hardware and framework, differences in reproduced results are possible.

### ä¸­æ–‡è¯„æµ‹ï¼ˆChinese Evaluationï¼‰

#### C-Eval

åœ¨[C-Eval](https://arxiv.org/abs/2305.08322)éªŒè¯é›†ä¸Šï¼Œæˆ‘ä»¬è¯„ä»·äº†Qwen-14B-Chatæ¨¡å‹çš„0-shot & 5-shotå‡†ç¡®ç‡

We demonstrate the 0-shot & 5-shot accuracy of Qwen-14B-Chat on C-Eval validation set

|              Model               | Avg. Acc. |
|:--------------------------------:|:---------:|
|          LLaMA2-7B-Chat          |   31.9    |
|         LLaMA2-13B-Chat          |   36.2    |
|         LLaMA2-70B-Chat          |   44.3    |
|         ChatGLM2-6B-Chat         |   52.6    |
|         InternLM-7B-Chat         |   53.6    |
|        Baichuan2-7B-Chat         |   55.6    |
|        Baichuan2-13B-Chat        |   56.7    |
| Qwen-7B-Chat (original) (0-shot) |   54.2    |
|    **Qwen-7B-Chat (0-shot)**     |   59.7    |
|    **Qwen-7B-Chat (5-shot)**     |   59.3    |
|    **Qwen-14B-Chat (0-shot)**    |   69.8    |
|    **Qwen-14B-Chat (5-shot)**    | **71.7**  |

C-Evalæµ‹è¯•é›†ä¸Šï¼ŒQwen-14B-Chatæ¨¡å‹çš„zero-shotå‡†ç¡®ç‡ç»“æœå¦‚ä¸‹ï¼š

The zero-shot accuracy of Qwen-14B-Chat on C-Eval testing set is provided below:

| Model                   |   Avg.   | STEM | Social Sciences | Humanities | Others |
| :---------------------- | :------: | :--: | :-------------: | :--------: | :----: |
| Chinese-Alpaca-Plus-13B |   41.5   | 36.6 |      49.7       |    43.1    |  41.2  |
| Chinese-Alpaca-2-7B     |   40.3   |  -   |        -        |     -      |   -    |
| ChatGLM2-6B-Chat        |   50.1   | 46.4 |      60.4       |    50.6    |  46.9  |
| Baichuan-13B-Chat       |   51.5   | 43.7 |      64.6       |    56.2    |  49.2  |
| Qwen-7B-Chat (original) |   54.6   | 47.8 |      67.6       |    59.3    |  50.6  |
| **Qwen-7B-Chat**        |   58.6   | 53.3 |      72.1       |    62.8    |  52.0  |
| **Qwen-14B-Chat**       | **69.1** | 65.1 |      80.9       |    71.2    |  63.4  |

åœ¨14Bè§„æ¨¡æ¨¡å‹ä¸Šï¼Œç»è¿‡äººç±»æŒ‡ä»¤å¯¹é½çš„Qwen-14B-Chatæ¨¡å‹ï¼Œå‡†ç¡®ç‡åœ¨åŒç±»ç›¸è¿‘è§„æ¨¡æ¨¡å‹ä¸­ä»ç„¶å¤„äºå‰åˆ—ã€‚

Compared with other pretrained models with comparable model size, the human-aligned Qwen-14B-Chat performs well in C-Eval accuracy.

### è‹±æ–‡è¯„æµ‹ï¼ˆEnglish Evaluationï¼‰

#### MMLU

[MMLU](https://arxiv.org/abs/2009.03300)è¯„æµ‹é›†ä¸Šï¼ŒQwen-14B-Chatæ¨¡å‹çš„ 0-shot & 5-shot å‡†ç¡®ç‡å¦‚ä¸‹ï¼Œæ•ˆæœåŒæ ·åœ¨åŒç±»å¯¹é½æ¨¡å‹ä¸­åŒæ ·è¡¨ç°è¾ƒä¼˜ã€‚

The 0-shot & 5-shot accuracy of Qwen-14B-Chat on MMLU is provided below.
The performance of Qwen-14B-Chat still on the top between other human-aligned models with comparable size.

|              Model               | Avg. Acc. |
|:--------------------------------:|:---------:|
|         ChatGLM2-6B-Chat         |   46.0    |
|          LLaMA2-7B-Chat          |   46.2    |
|         InternLM-7B-Chat         |   51.1    |
|        Baichuan2-7B-Chat         |   52.9    |
|         LLaMA2-13B-Chat          |   54.6    |
|        Baichuan2-13B-Chat        |   57.3    |
|         LLaMA2-70B-Chat          |   63.8    |
| Qwen-7B-Chat (original) (0-shot) |   53.9    |
|    **Qwen-7B-Chat (0-shot)**     |   55.8    |
|    **Qwen-7B-Chat (5-shot)**     |   57.0    |
|    **Qwen-14B-Chat (0-shot)**    |   64.6    |
|    **Qwen-14B-Chat (5-shot)**    | **66.5**  |

### ä»£ç è¯„æµ‹ï¼ˆCoding Evaluationï¼‰

Qwen-14B-Chatåœ¨[HumanEval](https://github.com/openai/human-eval)çš„zero-shot Pass@1æ•ˆæœå¦‚ä¸‹

The zero-shot Pass@1 of Qwen-14B-Chat on [HumanEval](https://github.com/openai/human-eval) is demonstrated below

|          Model          |  Pass@1  |
|:-----------------------:|:--------:|
|    ChatGLM2-6B-Chat     |   11.0   |
|     LLaMA2-7B-Chat      |   12.2   |
|    InternLM-7B-Chat     |   14.6   |
|    Baichuan2-7B-Chat    |   13.4   |
|     LLaMA2-13B-Chat     |   18.9   |
|   Baichuan2-13B-Chat    |   17.7   |
|     LLaMA2-70B-Chat     |   32.3   |
| Qwen-7B-Chat (original) |   24.4   |
|    **Qwen-7B-Chat**     |   37.2   |
|    **Qwen-14B-Chat**    | **43.9** |

### æ•°å­¦è¯„æµ‹ï¼ˆMathematics Evaluationï¼‰

åœ¨è¯„æµ‹æ•°å­¦èƒ½åŠ›çš„[GSM8K](https://github.com/openai/grade-school-math)ä¸Šï¼ŒQwen-14B-Chatçš„å‡†ç¡®ç‡ç»“æœå¦‚ä¸‹

The accuracy of Qwen-14B-Chat on GSM8K is shown below

|              Model               |   Acc.   |
|:--------------------------------:|:--------:|
|          LLaMA2-7B-Chat          |   26.3   |
|         ChatGLM2-6B-Chat         |   28.8   |
|        Baichuan2-7B-Chat         |   32.8   |
|         InternLM-7B-Chat         |   33.0   |
|         LLaMA2-13B-Chat          |   37.1   |
|        Baichuan2-13B-Chat        |   55.3   |
|         LLaMA2-70B-Chat          |   59.3   |
| Qwen-7B-Chat (original) (0-shot) |   41.1   |
|    **Qwen-7B-Chat (0-shot)**     |   50.3   |
|    **Qwen-7B-Chat (8-shot)**     |   54.1   |
|    **Qwen-14B-Chat (0-shot)**    | **60.1** |
|    **Qwen-14B-Chat (8-shot)**    |   59.3   |

### é•¿åºåˆ—è¯„æµ‹ï¼ˆLong-Context Understandingï¼‰

é€šè¿‡NTKæ’å€¼ï¼ŒLogNæ³¨æ„åŠ›ç¼©æ”¾å¯ä»¥æ‰©å±•Qwen-14B-Chatçš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚åœ¨é•¿æ–‡æœ¬æ‘˜è¦æ•°æ®é›†[VCSUM](https://arxiv.org/abs/2305.05280)ä¸Šï¼ˆæ–‡æœ¬å¹³å‡é•¿åº¦åœ¨15Kå·¦å³ï¼‰ï¼ŒQwen-14B-Chatçš„Rouge-Lç»“æœå¦‚ä¸‹ï¼š

**(è‹¥è¦å¯ç”¨è¿™äº›æŠ€å·§ï¼Œè¯·å°†config.jsoné‡Œçš„`use_dynamic_ntk`å’Œ`use_logn_attn`è®¾ç½®ä¸ºtrue)**

We introduce NTK-aware interpolation, LogN attention scaling to extend the context length of Qwen-14B-Chat. The Rouge-L results of Qwen-14B-Chat on long-text summarization dataset [VCSUM](https://arxiv.org/abs/2305.05280) (The average length of this dataset is around 15K) are shown below:

**(To use these tricks, please set `use_dynamic_ntk` and `use_long_attn` to true in config.json.)**

| Model             | VCSUM (zh) |
|:------------------|:----------:|
| GPT-3.5-Turbo-16k |    16.0    |
| LLama2-7B-Chat    |    0.2     |
| InternLM-7B-Chat  |    13.0    |
| ChatGLM2-6B-Chat  |    16.3    |
| **Qwen-14B-Chat** |  **17.3**  |


### å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„è¯„æµ‹ï¼ˆTool Usageï¼‰

#### ReAct Prompting

åƒé—®æ”¯æŒé€šè¿‡ [ReAct Prompting](https://arxiv.org/abs/2210.03629) è°ƒç”¨æ’ä»¶/å·¥å…·/APIã€‚ReAct ä¹Ÿæ˜¯ [LangChain](https://python.langchain.com/) æ¡†æ¶é‡‡ç”¨çš„ä¸»è¦æ–¹å¼ä¹‹ä¸€ã€‚åœ¨æˆ‘ä»¬å¼€æºçš„ã€ç”¨äºè¯„ä¼°å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„è¯„æµ‹åŸºå‡†ä¸Šï¼Œåƒé—®çš„è¡¨ç°å¦‚ä¸‹ï¼š

Qwen-Chat supports calling plugins/tools/APIs through [ReAct Prompting](https://arxiv.org/abs/2210.03629). ReAct is also one of the main approaches used by the [LangChain](https://python.langchain.com/) framework. In our evaluation benchmark for assessing tool usage capabilities, Qwen-Chat's performance is as follows:

<table>
    <tr>
        <th colspan="4" align="center">Chinese Tool-Use Benchmark</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selection (Acc.â†‘)</th><th align="center">Tool Input (Rouge-Lâ†‘)</th><th align="center">False Positive Errorâ†“</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">95%</td><td align="center">0.90</td><td align="center">15.0%</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">85%</td><td align="center">0.88</td><td align="center">75.0%</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">98%</td><td align="center">0.91</td><td align="center">7.3%</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">98%</td><td align="center">0.93</td><td align="center">2.4%</td>
    </tr>
</table>

> è¯„æµ‹åŸºå‡†ä¸­å‡ºç°çš„æ’ä»¶å‡æ²¡æœ‰å‡ºç°åœ¨åƒé—®çš„è®­ç»ƒé›†ä¸­ã€‚è¯¥åŸºå‡†è¯„ä¼°äº†æ¨¡å‹åœ¨å¤šä¸ªå€™é€‰æ’ä»¶ä¸­é€‰æ‹©æ­£ç¡®æ’ä»¶çš„å‡†ç¡®ç‡ã€ä¼ å…¥æ’ä»¶çš„å‚æ•°çš„åˆç†æ€§ã€ä»¥åŠå‡é˜³ç‡ã€‚å‡é˜³ç‡ï¼ˆFalse Positiveï¼‰å®šä¹‰ï¼šåœ¨å¤„ç†ä¸è¯¥è°ƒç”¨æ’ä»¶çš„è¯·æ±‚æ—¶ï¼Œé”™è¯¯åœ°è°ƒç”¨äº†æ’ä»¶ã€‚

> The plugins that appear in the evaluation set do not appear in the training set of Qwen. This benchmark evaluates the accuracy of the model in selecting the correct plugin from multiple candidate plugins, the rationality of the parameters passed into the plugin, and the false positive rate. False Positive: Incorrectly invoking a plugin when it should not have been called when responding to a query.

![](assets/react_showcase_001.png)
![](assets/react_showcase_002.png)

#### Code Interpreter

ä¸ºäº†è€ƒå¯ŸQwenä½¿ç”¨Python Code Interpreterå®Œæˆæ•°å­¦è§£é¢˜ã€æ•°æ®å¯è§†åŒ–ã€åŠæ–‡ä»¶å¤„ç†ä¸çˆ¬è™«ç­‰ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸“é—¨å»ºè®¾å¹¶å¼€æºäº†ä¸€ä¸ªè¯„æµ‹è¿™æ–¹é¢èƒ½åŠ›çš„[è¯„æµ‹åŸºå‡†](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark)ã€‚

æˆ‘ä»¬å‘ç°Qwenåœ¨ç”Ÿæˆä»£ç çš„å¯æ‰§è¡Œç‡ã€ç»“æœæ­£ç¡®æ€§ä¸Šå‡è¡¨ç°è¾ƒå¥½ï¼š

To assess Qwen's ability to use the Python Code Interpreter for tasks such as mathematical problem solving, data visualization, and other general-purpose tasks such as file handling and web scraping, we have created and open-sourced a benchmark specifically designed for evaluating these capabilities. You can find the benchmark at this [link](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).

We have observed that Qwen performs well in terms of code executability and result accuracy when generating code:

<table>
    <tr>
        <th colspan="4" align="center">Executable Rate of Generated Code (%)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Mathâ†‘</th><th align="center">Visualizationâ†‘</th><th align="center">Generalâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">91.9</td><td align="center">85.9</td><td align="center">82.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">89.2</td><td align="center">65.0</td><td align="center">74.1</td>
    </tr>
    <tr>
        <td>LLaMA2-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">33.1</td>
        <td align="center">24.1 </td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">50.0</td>
        <td align="center">40.5</td>
        <td align="center">48.3 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-7B-Instruct</td>
        <td align="center">85.1</td>
        <td align="center">54.0</td>
        <td align="center">70.7 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">93.2</td>
        <td align="center">55.8</td>
        <td align="center">74.1 </td>
    </tr>
    <tr>
        <td>InternLM-7B-Chat-v1.1</td>
        <td align="center">78.4</td>
        <td align="center">44.2</td>
        <td align="center">62.1 </td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">70.3</td>
        <td align="center">44.2</td>
        <td align="center">65.5 </td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">82.4</td>
        <td align="center">64.4</td>
        <td align="center">67.2 </td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">89.2</td>
        <td align="center">84.1</td>
        <td align="center">65.5</td>
    </tr>
</table>

<table>
    <tr>
        <th colspan="4" align="center">Accuracy of Code Execution Results (%)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Mathâ†‘</th><th align="center">Visualization-Hardâ†‘</th><th align="center">Visualization-Easyâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">82.8</td><td align="center">66.7</td><td align="center">60.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">47.3</td><td align="center">33.3</td><td align="center">55.7</td>
    </tr>
    <tr>
        <td>LLaMA2-7B-Chat</td>
        <td align="center">3.9</td>
        <td align="center">14.3</td>
        <td align="center">39.2 </td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">8.3</td>
        <td align="center">8.3</td>
        <td align="center">40.5 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-7B-Instruct</td>
        <td align="center">14.3</td>
        <td align="center">26.2</td>
        <td align="center">60.8 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">28.2</td>
        <td align="center">27.4</td>
        <td align="center">62.0 </td>
    </tr>
    <tr>
        <td>InternLM-7B-Chat-v1.1</td>
        <td align="center">28.5</td>
        <td align="center">4.8</td>
        <td align="center">40.5 </td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">34.6</td>
        <td align="center">21.4</td>
        <td align="center">45.6 </td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">40.5</td>
        <td align="center">54.4 </td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">58.4</td>
        <td align="center">53.6</td>
        <td align="center">59.5</td>
    </tr>
</table>

<p align="center">
    <br>
    <img src="assets/code_interpreter_showcase_001.jpg" />
    <br>
<p>

#### Huggingface Agent

åƒé—®è¿˜å…·å¤‡ä½œä¸º [HuggingFace Agent](https://huggingface.co/docs/transformers/transformers_agents) çš„èƒ½åŠ›ã€‚å®ƒåœ¨ Huggingface æä¾›çš„runæ¨¡å¼è¯„æµ‹åŸºå‡†ä¸Šçš„è¡¨ç°å¦‚ä¸‹ï¼š

Qwen-Chat also has the capability to be used as a [HuggingFace Agent](https://huggingface.co/docs/transformers/transformers_agents). Its performance on the run-mode benchmark provided by HuggingFace is as follows:

<table>
    <tr>
        <th colspan="4" align="center">HuggingFace Agent Benchmark- Run Mode</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selectionâ†‘</th><th align="center">Tool Usedâ†‘</th><th align="center">Codeâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">100</td><td align="center">100</td><td align="center">97.4</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">95.4</td><td align="center">96.3</td><td align="center">87.0</td>
    </tr>
    <tr>
        <td>StarCoder-Base-15B</td><td align="center">86.1</td><td align="center">87.0</td><td align="center">68.9</td>
    </tr>
    <tr>
        <td>StarCoder-15B</td><td align="center">87.0</td><td align="center">88.0</td><td align="center">68.9</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">87.0</td><td align="center">87.0</td><td align="center">71.5</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">93.5</td><td align="center">94.4</td><td align="center">87.0</td>
    </tr>
</table>

<table>
    <tr>
        <th colspan="4" align="center">HuggingFace Agent Benchmark - Chat Mode</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selectionâ†‘</th><th align="center">Tool Usedâ†‘</th><th align="center">Codeâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">98.5</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">97.3</td><td align="center">96.8</td><td align="center">89.6</td>
    </tr>
    <tr>
        <td>StarCoder-Base-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">91.1</td>
    </tr>
    <tr>
        <td>StarCoder-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">89.6</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">94.7</td><td align="center">94.7</td><td align="center">85.1</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">95.5</td>
    </tr>
</table>

<br>

## FAQ

å¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜…[FAQ](https://github.com/QwenLM/Qwen/blob/main/FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚

If you meet problems, please refer to [FAQ](https://github.com/QwenLM/Qwen/blob/main/FAQ.md) and the issues first to search a solution before you launch a new issue.
<br>

## å¼•ç”¨ (Citation)

å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ï¼

If you find our work helpful, feel free to give us a cite.

```
@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
```
<br>

## ä½¿ç”¨åè®®ï¼ˆLicense Agreementï¼‰

æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¹¶æ”¯æŒå•†ç”¨ã€‚è¯·æŸ¥çœ‹[LICENSE](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT)äº†è§£å…·ä½“çš„å¼€æºåè®®ç»†èŠ‚ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·å¡«å†™[é—®å·](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat)ç”³è¯·ã€‚

Our code and checkpoints are open to research purpose, and they are allowed for commercial purposes. Check [LICENSE](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) for more details about the license. If you have requirements for commercial use, please fill out the [form](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat) to apply.
<br>



## è”ç³»æˆ‘ä»¬ï¼ˆContact Usï¼‰

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œæ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤ã€é’‰é’‰ç¾¤ä»¥åŠDiscordï¼åŒæ—¶ï¼Œä¹Ÿæ¬¢è¿é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚

If you are interested to leave a message to either our research team or product team, join our Discord or WeChat groups! Also, feel free to send an email to qianwen_opensource@alibabacloud.com.

